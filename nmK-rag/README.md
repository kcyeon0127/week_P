# 국립중앙박물관 RAG 데이터 구축 프로젝트

이 프로젝트는 국립중앙박물관 웹사이트의 데이터를 수집하고 가공하여, 검색 증강 생성(RAG) 기반의 챗봇을 위한 데이터셋을 구축하는 것을 목표로 합니다.

## 디렉토리 구조

- **/data_raw**: `crawl.py`를 통해 수집된 원본 웹페이지 데이터(JSON 형식)가 저장되는 곳입니다.
- **/index**: `embed_index.py`를 통해 생성된 벡터 인덱스가 저장되는 곳입니다.
- **/src**: 데이터 수집, 정제, 임베딩 등 핵심 로직을 담은 파이썬 스크립트가 위치합니다.

## 스크립트 설명

### `src/crawl.py`

국립중앙박물관 웹사이트의 콘텐츠를 지능적으로 크롤링하는 스크립트입니다.

#### 주요 기능

- **지능형 탐색**: 단순 링크를 따라가는 것이 아니라, 사전에 정의된 규칙에 따라 필요한 페이지만 선별적으로 탐색하고 수집합니다.
  - **상설 전시**: `ALLOWED_HALL_IDS`에 정의된 전시관 ID(`showHallId`)를 기반으로 모든 하위 전시실과 그 안의 전시품(`relicId`) 상세 페이지를 수집합니다.
  - **특별 전시**: `exhiSpThemId`가 포함된 링크를 따라가 현재 진행중인 특별 전시의 상세 정보를 수집합니다.
  - **지난 전시**: '지난 전시' 목록 페이지에서 가장 최근 N개의 전시를 자동으로 찾아내어 수집 목록에 추가합니다.
- **Polite Crawling**: 서버에 부담을 주지 않기 위해 각 요청 사이에 1초의 지연 시간을 둡니다.

#### 설정 방법

스크립트 상단의 `[설정 영역]`에서 아래 변수들을 수정하여 크롤링 대상을 제어할 수 있습니다.

- `SEED_URLS`: 크롤링을 시작할 최상위 페이지 목록입니다. (예: 상설전시 층별안내, 현재 전시 등)
- `ALLOWED_HALL_IDS`: 수집을 허용할 상설 전시관의 고유 ID 목록입니다.
- `PAST_EXHIBITION_LIMIT`: 수집할 '지난 전시'의 최대 개수입니다.

### `src/clean_chunk.py`

`data_raw`에 저장된 원본 JSON 파일들을 읽어와 RAG 모델이 사용하기 좋은 형태로 가공하는 스크립트입니다.

- **주요 기능**:
  - `split_into_chunks` 함수를 통해 문서를 의미있는 단위의 청크(Chunk)로 분할합니다.
  - 문단 단위로 먼저 분할하고, 청크의 최소/최대 길이에 맞춰 동적으로 단락을 합치거나 문장 단위로 나누는 정교한 로직을 사용합니다.
  - 최종 결과물은 `data_curated/chunks.jsonl` 파일로 저장됩니다.

### `src/embed_index.py`

`clean_chunk.py`를 통해 생성된 `chunks.jsonl` 파일을 읽어, 각 텍스트 청크를 벡터로 변환하고 이를 벡터 데이터베이스에 저장하여 인덱스를 생성합니다.

- **주요 기능**:
  - `sentence-transformers` 라이브러리(`BAAI/bge-m3` 모델)를 사용하여 텍스트 임베딩을 생성합니다.
  - `ChromaDB`를 벡터 데이터베이스로 사용하며, `index/chroma` 디렉토리에 인덱스를 저장합니다.
  - 스크립트 실행 시 기존 컬렉션이 있다면 삭제하고 새로 생성하여 항상 최신 상태의 인덱스를 유지합니다.

### `app.py`

`Streamlit`으로 제작된 RAG 챗봇 데모 웹 애플리케이션입니다. 사용자가 질문을 입력하고, RAG 파이프라인을 통해 생성된 답변과 근거를 확인할 수 있습니다.

- **주요 기능**:
  - `src.rag_chain`의 `RAG` 클래스를 사용하여 검색-생성 파이프라인을 실행합니다.
  - 답변의 근거가 된 컨텍스트와 원문 출처(URL)를 함께 표시합니다.
  - 답변에 대한 정확성, 충분성 등을 평가하고 코멘트를 저장하는 기능을 포함합니다.

## 전체 데이터 구축 워크플로우

1.  **의존성 설치**:
    ```bash
    pip install -r requirements.txt
    ```

2.  **데이터 수집 실행**:
    - (선택) `src/crawl.py` 파일을 열어 크롤링 대상을 수정합니다.
    ```bash
    python src/crawl.py
    ```
    - 실행이 완료되면 `data_raw` 폴더에 JSON 파일들이 생성됩니다.

3.  **데이터 정제 및 분할 실행**:
    ```bash
    python src/clean_chunk.py data_raw
    ```
    - `data_raw` 폴더의 모든 json을 처리하여 `data_curated/chunks.jsonl` 파일을 생성합니다.

4.  **임베딩 및 인덱스 생성 실행**:
    ```bash
    python src/embed_index.py
    ```
    - `chunks.jsonl` 파일을 읽어 `index/chroma` 디렉토리에 벡터 인덱스를 생성합니다.

5.  **애플리케이션 실행**:
    ```bash
    streamlit run app.py
    ```
